\section{Related Work} \label{sec:related_work}

\paragraph{Transform Coding}
Transform coding is a widely used approach in lossy image compression, leveraging mathematical transforms to decorrelate pixel values and represent image data more compactly. One of the earliest and most influential methods is the discrete cosine transform (DCT) \cite{ahmed1974discrete}, used in JPEG \cite{wallace1991jpeg}, which converts image data into the frequency domain, prioritizing lower frequencies to retain perceptually significant information. The discrete wavelet transform (DWT) \cite{antonini1992image}, used in JPEG 2000 \cite{skodras2001jpeg}, offers improved performance by capturing both frequency and location information, leading to better handling of edges and textures \cite{shapiro1993embedded}. More recently, the WebP \cite{google2011webp} and HEIF \cite{lainema2016hevc, hannuksela2015high} formats combine DCT and intra-frame prediction to achieve superior compression and quality compared to JPEG.

\paragraph{Learned Image Compression (LIC)}
Recently, learned image compression (LIC) has gained attention for potentially outperforming traditional methods by leveraging deep neural networks. \citet{balle2018variational} pioneered this area with an end-to-end trainable convolutional neural network based on variational autoencoders.\remove{Mentzer et al. (2020) ("High-fidelity generative image compression") integrated generative adversarial networks into LIC, developing an effective generative lossy compression system.} \citet{cheng2020learned} incorporated a simplified attention module and discretized Gaussian mixture likelihoods for achieving a more accurate and flexible entropy model. \citet{liu2023learned} combined transformers and CNNs to exploit the local modeling ability of convolutions and the global modeling ability of the attention mechanism. \citet{yang2024lossy} introduced diffusion models into LIC, using a denoising decoder to iteratively reconstruct a compressed image. Despite these advancements, the high computational complexity of LIC methods remains a significant limitation, particularly for real-time applications and resource-constrained environments.

\paragraph{Low-rank Techniques}
Low-rank approximation can provide a compact representation by decomposing image data into smaller components. Notably, truncated singular value decomposition (tSVD) is a classical technique that decomposes images into singular values and vectors, retaining only the most significant components to achieve compression \cite{andrews1976singular, prasantha2007image}. \citet{hou2015sparse} proposed sparse low-rank matrix approximation (SLRMA) for data compression, which is able to explore both the intra- and inter-coherence of data samples simultaneously from the perspective of optimization and transformation.
More recently, \citet{yuan2020image} introduced a graph-based low-rank regularization to reduce compression artifacts near block boundaries at low bit rates.

\paragraph{Integer Matrix Factorization}
There are applications where meaningful representation of data as discrete factor matrices is crucial. While typical low-rank techniques like SVD and nonnegative matrix factorization (NMF) are inappropriate for such applications, integer matrix factorization (IMF) ensures the integrality of factors to achieve this goal. \citet{lin2005integer} investigates IMF to effectively handle discrete data matrices for cluster analysis and pattern discovery. \citet{dong2018integer} introduce an alternative least squares method for IMF, verifying its effectiveness with some data mining applications. However, the application of IMF in image compression remains unexplored.

While existing IMF methods generally constrain factor elements to the entire set of integer values, we propose quantized matrix factorization (QMF), which minimizes the objective function over a bounded interval of integers, thereby modeling a uniformly quantized domain. Furthermore, we introduce a block coordinate descent (BCD)-based algorithm to solve the QMF problem, which is both computationally efficient and provably convergent. This work investigates the potential of QMF for image compression, arguing that it can serve as a powerful tool for this purpose.